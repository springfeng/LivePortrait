# coding: utf-8

"""
Pipeline of LivePortrait
"""

import torch

from .utils.viz import viz_lmk

torch.backends.cudnn.benchmark = True  # disable CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR warning

import cv2;

cv2.setNumThreads(0);
cv2.ocl.setUseOpenCL(False)
import numpy as np
import os
import os.path as osp
from rich.progress import track

from .config.argument_config import ArgumentConfig
from .config.inference_config import InferenceConfig
from .config.crop_config import CropConfig
from .utils.cropper import Cropper
from .utils.camera import get_rotation_matrix
from .utils.video import images2video, concat_frames, get_fps, add_audio_to_video, has_audio_stream
from .utils.crop import _transform_img, prepare_paste_back, paste_back
from .utils.io import load_image_rgb, load_driving_info, resize_to_limit, dump, load
from .utils.helper import mkdir, basename, dct2device, is_video, is_template, remove_suffix
from .utils.rprint import rlog as log
# from .utils.viz import viz_lmk
from .live_portrait_wrapper import LivePortraitWrapper


def make_abs_path(fn):
    return osp.join(osp.dirname(osp.realpath(__file__)), fn)


class LivePortraitPipeline(object):

    def __init__(self, inference_cfg: InferenceConfig, crop_cfg: CropConfig):
        self.live_portrait_wrapper: LivePortraitWrapper = LivePortraitWrapper(inference_cfg=inference_cfg)
        self.cropper: Cropper = Cropper(crop_cfg=crop_cfg)

    def execute(self, args: ArgumentConfig):
        inf_cfg = self.live_portrait_wrapper.inference_cfg  # æ¨ç†é…ç½®
        device = self.live_portrait_wrapper.device  # è®¾å¤‡ä¿¡æ¯
        crop_cfg = self.cropper.crop_cfg  # è£å‰ªé…ç½®

        # å¤„ç†æºè‚–åƒå›¾åƒ
        img_rgb = load_image_rgb(args.source_image)  # åŠ è½½RGBæ ¼å¼çš„æºå›¾åƒ
        img_rgb = resize_to_limit(img_rgb, inf_cfg.source_max_dim, inf_cfg.source_division)  # å°†å›¾åƒè°ƒæ•´åˆ°æœ€å¤§å°ºå¯¸é™åˆ¶
        log(f"ä» {args.source_image} åŠ è½½æºå›¾åƒ")  # æ—¥å¿—è®°å½•åŠ è½½çš„æºå›¾åƒè·¯å¾„

        # ä½¿ç”¨è£å‰ªé…ç½®è£å‰ªæºå›¾åƒ
        crop_info = self.cropper.crop_source_image(img_rgb, crop_cfg)
        if crop_info is None:  # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°äººè„¸
            raise Exception("åœ¨æºå›¾åƒä¸­æœªæ£€æµ‹åˆ°äººè„¸!")  # æŠ›å‡ºå¼‚å¸¸

        source_lmk = crop_info['lmk_crop']  # è·å–è£å‰ªåçš„äººè„¸å…³é”®ç‚¹
        img_crop, img_crop_256x256 = crop_info['img_crop'], crop_info['img_crop_256x256']  # è£å‰ªåçš„å›¾åƒå’Œå›ºå®šå¤§å°ï¼ˆ256x256ï¼‰çš„å›¾åƒ
        # ä¿å­˜è£å‰ªåçš„å›¾åƒä¸º img_crop.jpg
        # è¾“å‡ºæ—¥å¿—
        log(f"è£å‰ªåçš„å›¾åƒå·²ä¿å­˜ä¸º img_crop.jpgã€‚")
        cv2.imwrite("img_crop.jpg", img_crop)
        cv2.imwrite("img_crop_256x256.jpg", img_crop_256x256)

        # å¦‚æœéœ€è¦è£å‰ªï¼Œå‡†å¤‡è£å‰ªåçš„å›¾åƒï¼›å¦åˆ™ï¼Œå°†åŸå§‹å›¾åƒå¼ºåˆ¶ç¼©æ”¾åˆ°256x256
        if inf_cfg.flag_do_crop:
            I_s = self.live_portrait_wrapper.prepare_source(img_crop_256x256)
        else:
            img_crop_256x256 = cv2.resize(img_rgb, (256, 256))
            I_s = self.live_portrait_wrapper.prepare_source(img_crop_256x256)

        # è·å–å…³é”®ç‚¹ä¿¡æ¯
        x_s_info = self.live_portrait_wrapper.get_kp_info(I_s)
        x_c_s = x_s_info['kp']  # å…³é”®ç‚¹ä½ç½®
        R_s = get_rotation_matrix(x_s_info['pitch'], x_s_info['yaw'], x_s_info['roll'])  # è·å–æ—‹è½¬çŸ©é˜µ
        f_s = self.live_portrait_wrapper.extract_feature_3d(I_s)  # æå–3Dç‰¹å¾
        x_s = self.live_portrait_wrapper.transform_keypoint(x_s_info)  # è½¬æ¢å…³é”®ç‚¹

        # åˆ¤æ–­æ˜¯å¦éœ€è¦å°†å˜´å”‡æ¯”ä¾‹è®¾ç½®ä¸ºé›¶
        flag_lip_zero = inf_cfg.flag_lip_zero  # æ˜¯å¦å°†å˜´å”‡å¼ å¼€åº¦è®¾ä¸º0
        if flag_lip_zero:
            # åœ¨åŠ¨ç”»ä¹‹å‰è®©å˜´å”‡å¼ å¼€åº¦ä¸º0
            c_d_lip_before_animation = [0.]
            combined_lip_ratio_tensor_before_animation = self.live_portrait_wrapper.calc_combined_lip_ratio(
                c_d_lip_before_animation, source_lmk)
            lip_zero_threshold = combined_lip_ratio_tensor_before_animation[0][0]  # è®¡ç®—å˜´å”‡å¼ å¼€åº¦é˜ˆå€¼
            if lip_zero_threshold < inf_cfg.lip_zero_threshold:  # å¦‚æœä½äºè®¾å®šçš„é˜ˆå€¼ï¼Œåˆ™ä¸æ‰§è¡Œè®¾é›¶æ“ä½œ
                flag_lip_zero = False
            else:
                lip_delta_before_animation = self.live_portrait_wrapper.retarget_lip(x_s,
                                                                                     combined_lip_ratio_tensor_before_animation)  # å¦åˆ™ï¼Œé‡æ–°å®šä½å˜´å”‡å…³é”®ç‚¹

        # å¤„ç†é©±åŠ¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä»æ¨¡æ¿åŠ è½½æˆ–è§†é¢‘æ–‡ä»¶ä¸­æå–æ•°æ®
        flag_load_from_template = is_template(args.driving_info)  # åˆ¤æ–­æ˜¯å¦ä»æ¨¡æ¿åŠ è½½
        driving_rgb_crop_256x256_lst = None  # åˆå§‹åŒ–è£å‰ªåçš„RGBå¸§åˆ—è¡¨
        wfp_template = None  # åˆå§‹åŒ–æ¨¡æ¿æ–‡ä»¶è·¯å¾„

        if flag_load_from_template:
            # å¦‚æœæ˜¯ä»æ¨¡æ¿åŠ è½½ï¼Œåˆ™å¿«é€ŸåŠ è½½æ¨¡æ¿ä¿¡æ¯ï¼Œä½†è§†é¢‘å’ŒéŸ³é¢‘è£å‰ªæ— æ•ˆ
            log(f"ä»æ¨¡æ¿åŠ è½½ï¼š{args.driving_info}ï¼Œä¸æ˜¯è§†é¢‘ï¼Œæ‰€ä»¥è£å‰ªçš„è§†é¢‘å’ŒéŸ³é¢‘éƒ½æ˜¯NULLã€‚", style='bold green')
            template_dct = load(args.driving_info)  # åŠ è½½æ¨¡æ¿å­—å…¸
            n_frames = template_dct['n_frames']  # è·å–å¸§æ•°

            # è®¾ç½®è¾“å‡ºå¸§ç‡
            output_fps = template_dct.get('output_fps', inf_cfg.output_fps)
            log(f'æ¨¡æ¿çš„FPSï¼š{output_fps}')

            if args.flag_crop_driving_video:
                log("è­¦å‘Šï¼šflag_crop_driving_videoä¸ºçœŸï¼Œä½†é©±åŠ¨ä¿¡æ¯æ¥è‡ªæ¨¡æ¿ï¼Œå› æ­¤è¯¥é€‰é¡¹è¢«å¿½ç•¥ã€‚")

        elif osp.exists(args.driving_info) and is_video(args.driving_info):
            # å¦‚æœä»è§†é¢‘æ–‡ä»¶åŠ è½½ï¼Œå¹¶åˆ›å»ºè¿åŠ¨æ¨¡æ¿
            log(f"åŠ è½½è§†é¢‘ï¼š{args.driving_info}")
            if osp.isdir(args.driving_info):
                output_fps = inf_cfg.output_fps
            else:
                output_fps = int(get_fps(args.driving_info))  # è·å–è§†é¢‘å¸§ç‡
                log(f'{args.driving_info}çš„FPSæ˜¯ï¼š{output_fps}')

            log(f"åŠ è½½è§†é¢‘æ–‡ä»¶ (mp4 mov aviç­‰...)ï¼š{args.driving_info}")
            driving_rgb_lst = load_driving_info(args.driving_info)  # åŠ è½½é©±åŠ¨ä¿¡æ¯

            # å¼€å§‹åˆ›å»ºè¿åŠ¨æ¨¡æ¿
            log("å¼€å§‹åˆ›å»ºè¿åŠ¨æ¨¡æ¿...")
            if inf_cfg.flag_crop_driving_video:
                ret = self.cropper.crop_driving_video(driving_rgb_lst)  # è£å‰ªè§†é¢‘å¸§
                log(f'é©±åŠ¨è§†é¢‘å·²è£å‰ªï¼Œå¤„ç†äº†{len(ret["frame_crop_lst"])}å¸§ã€‚')
                driving_rgb_crop_lst, driving_lmk_crop_lst = ret['frame_crop_lst'], ret['lmk_crop_lst']  # åˆ†ç¦»è£å‰ªåçš„å¸§å’Œå…³é”®ç‚¹
                driving_rgb_crop_256x256_lst = [cv2.resize(_, (256, 256)) for _ in
                                                driving_rgb_crop_lst]  # å°†è£å‰ªåçš„å¸§ç¼©æ”¾åˆ°256x256
            else:
                driving_lmk_crop_lst = self.cropper.calc_lmks_from_cropped_video(driving_rgb_lst)  # ä»è£å‰ªè§†é¢‘è®¡ç®—å…³é”®ç‚¹
                driving_rgb_crop_256x256_lst = [cv2.resize(_, (256, 256)) for _ in driving_rgb_lst]  # å¼ºåˆ¶ç¼©æ”¾æ‰€æœ‰å¸§åˆ°256x256

            c_d_eyes_lst, c_d_lip_lst = self.live_portrait_wrapper.calc_driving_ratio(
                driving_lmk_crop_lst)  # è®¡ç®—çœ¼ç›å’Œå˜´å”‡çš„æ¯”ä¾‹
            # å‡†å¤‡é©±åŠ¨è§†é¢‘
            I_d_lst = self.live_portrait_wrapper.prepare_driving_videos(driving_rgb_crop_256x256_lst)
            template_dct = self.make_motion_template(I_d_lst, c_d_eyes_lst, c_d_lip_lst,
                                                     output_fps=output_fps)  # åˆ›å»ºè¿åŠ¨æ¨¡æ¿

            wfp_template = remove_suffix(args.driving_info) + '.pkl'  # æ„å»ºæ¨¡æ¿æ–‡ä»¶å
            dump(wfp_template, template_dct)  # ä¿å­˜æ¨¡æ¿åˆ°æ–‡ä»¶
            log(f"å°†è¿åŠ¨æ¨¡æ¿ä¿å­˜åˆ° {wfp_template}")

            n_frames = I_d_lst.shape[0]  # è·å–æ¨¡æ¿å¸§æ•°
        else:
            raise Exception(f"{args.driving_info}ä¸å­˜åœ¨æˆ–ä¸æ”¯æŒçš„é©±åŠ¨ä¿¡æ¯ç±»å‹ï¼")

        ######## prepare for pasteback ########
        # å‡†å¤‡ç”¨äºç²˜è´´å›å»ï¼ˆpastebackï¼‰çš„å¸§
        I_p_pstbk_lst = None
        if inf_cfg.flag_pasteback and inf_cfg.flag_do_crop and inf_cfg.flag_stitching:
            # å‡†å¤‡ç²˜è´´å›å»æ‰€éœ€è¦çš„æ©ç 
            mask_ori_float = prepare_paste_back(inf_cfg.mask_crop, crop_info['M_c2o'],
                                                dsize=(img_rgb.shape[1], img_rgb.shape[0]))
            I_p_pstbk_lst = []
            log("ç²˜è´´å›å»çš„æ©ç å‡†å¤‡å®Œæˆã€‚")
        #########################################

        I_p_lst = []
        R_d_0, x_d_0_info = None, None

        for i in track(range(n_frames), description='ğŸš€Animating...', total=n_frames):
            # åŠ è½½æ¯å¸§çš„è¿åŠ¨ä¿¡æ¯
            x_d_i_info = template_dct['motion'][i]
            x_d_i_info = dct2device(x_d_i_info, device)
            R_d_i = x_d_i_info['R_d']

            # ä¿å­˜ç¬¬ä¸€å¸§çš„è¿åŠ¨ä¿¡æ¯
            if i == 0:
                R_d_0 = R_d_i
                x_d_0_info = x_d_i_info

            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨ç›¸å¯¹è¿åŠ¨
            if inf_cfg.flag_relative_motion:
                # è®¡ç®—æ–°çš„æ—‹è½¬ã€deltaã€scaleå’Œt
                R_new = (R_d_i @ R_d_0.permute(0, 2, 1)) @ R_s
                delta_new = x_s_info['exp'] + (x_d_i_info['exp'] - x_d_0_info['exp'])
                scale_new = x_s_info['scale'] * (x_d_i_info['scale'] / x_d_0_info['scale'])
                t_new = x_s_info['t'] + (x_d_i_info['t'] - x_d_0_info['t'])
            else:
                R_new = R_d_i
                delta_new = x_d_i_info['exp']
                scale_new = x_s_info['scale']
                t_new = x_d_i_info['t']

            # è®¾ç½®tzä¸º0
            t_new[..., 2].fill_(0)
            # è®¡ç®—æ–°çš„å˜å½¢
            x_d_i_new = scale_new * (x_c_s @ R_new + delta_new) + t_new

            # æ ¹æ®ä¸åŒçš„é…ç½®è¿›è¡Œä¸åŒçš„æ“ä½œ
            if not inf_cfg.flag_stitching and not inf_cfg.flag_eye_retargeting and not inf_cfg.flag_lip_retargeting:
                # å¦‚æœä¸éœ€è¦ç¼åˆæˆ–é‡å®šå‘
                if flag_lip_zero:
                    x_d_i_new += lip_delta_before_animation.reshape(-1, x_s.shape[1], 3)
            elif inf_cfg.flag_stitching and not inf_cfg.flag_eye_retargeting and not inf_cfg.flag_lip_retargeting:
                # å¦‚æœéœ€è¦ç¼åˆä½†ä¸éœ€è¦é‡å®šå‘
                if flag_lip_zero:
                    x_d_i_new = self.live_portrait_wrapper.stitching(x_s,
                                                                     x_d_i_new) + lip_delta_before_animation.reshape(-1,
                                                                                                                     x_s.shape[
                                                                                                                         1],
                                                                                                                     3)
                else:
                    x_d_i_new = self.live_portrait_wrapper.stitching(x_s, x_d_i_new)
            else:
                # å¤„ç†çœ¼ç›å’Œå˜´å”‡çš„é‡å®šå‘
                eyes_delta, lip_delta = None, None
                if inf_cfg.flag_eye_retargeting:
                    eyes_delta = self.live_portrait_wrapper.retarget_eye(x_s, combined_eye_ratio_tensor)
                if inf_cfg.flag_lip_retargeting:
                    lip_delta = self.live_portrait_wrapper.retarget_lip(x_s, combined_lip_ratio_tensor)

                # æ›´æ–°å˜å½¢
                x_d_i_new = x_d_i_new + (eyes_delta.reshape(-1, x_s.shape[1], 3) if eyes_delta is not None else 0) + (
                    lip_delta.reshape(-1, x_s.shape[1], 3) if lip_delta is not None else 0)

                if inf_cfg.flag_stitching:
                    x_d_i_new = self.live_portrait_wrapper.stitching(x_s, x_d_i_new)

            # åº”ç”¨å˜å½¢å¹¶è§£ç 
            out = self.live_portrait_wrapper.warp_decode(f_s, x_s, x_d_i_new)
            I_p_i = self.live_portrait_wrapper.parse_output(out['out'])[0]
            I_p_lst.append(I_p_i)

            # å¦‚æœéœ€è¦ç²˜è´´å›å»
            if inf_cfg.flag_pasteback and inf_cfg.flag_do_crop and inf_cfg.flag_stitching:
                I_p_pstbk = paste_back(I_p_i, crop_info['M_c2o'], img_rgb, mask_ori_float)
                I_p_pstbk_lst.append(I_p_pstbk)

        # è¾“å‡ºç»“æœ
        mkdir(args.output_dir)
        wfp_concat = None
        flag_has_audio = (not flag_load_from_template) and has_audio_stream(args.driving_info)

        # æ‹¼æ¥æœ€ç»ˆç»“æœ
        frames_concatenated = concat_frames(driving_rgb_crop_256x256_lst, img_crop_256x256, I_p_lst)
        wfp_concat = osp.join(args.output_dir,
                              f'{basename(args.source_image)}--{basename(args.driving_info)}_concat.mp4')
        images2video(frames_concatenated, wfp=wfp_concat, fps=output_fps)

        # å¦‚æœæœ‰éŸ³é¢‘æµï¼Œæ·»åŠ éŸ³é¢‘
        if flag_has_audio:
            wfp_concat_with_audio = osp.join(args.output_dir,
                                             f'{basename(args.source_image)}--{basename(args.driving_info)}_concat_with_audio.mp4')
            add_audio_to_video(wfp_concat, args.driving_info, wfp_concat_with_audio)
            os.replace(wfp_concat_with_audio, wfp_concat)
            log(f"æ›¿æ¢ {wfp_concat} ä¸º {wfp_concat_with_audio}")

        # ä¿å­˜åŠ¨ç”»ç»“æœ
        wfp = osp.join(args.output_dir, f'{basename(args.source_image)}--{basename(args.driving_info)}.mp4')
        if I_p_pstbk_lst is not None and len(I_p_pstbk_lst) > 0:
            images2video(I_p_pstbk_lst, wfp=wfp, fps=output_fps)
        else:
            images2video(I_p_lst, wfp=wfp, fps=output_fps)

        # å¦‚æœæœ‰éŸ³é¢‘æµï¼Œå†æ¬¡æ·»åŠ éŸ³é¢‘
        if flag_has_audio:
            wfp_with_audio = osp.join(args.output_dir,
                                      f'{basename(args.source_image)}--{basename(args.driving_info)}_with_audio.mp4')
            add_audio_to_video(wfp, args.driving_info, wfp_with_audio)
            os.replace(wfp_with_audio, wfp)
            log(f"æ›¿æ¢ {wfp} ä¸º {wfp_with_audio}")

        # æœ€ç»ˆæ—¥å¿—
        log(f'åŠ¨ç”»æ¨¡æ¿: {wfp_template}, ä¸‹æ¬¡ä½ å¯ä»¥ä½¿ç”¨ `-d` å‚æ•°æŒ‡å®šè¿™ä¸ªæ¨¡æ¿è·¯å¾„ï¼Œé¿å…é‡æ–°è£å‰ªè§†é¢‘ï¼Œåˆ¶ä½œè¿åŠ¨æ¨¡æ¿å’Œä¿æŠ¤éšç§ã€‚')
        log(f'åŠ¨ç”»è§†é¢‘: {wfp}')
        log(f'æ‹¼æ¥åŠ¨ç”»è§†é¢‘: {wfp_concat}')

        return wfp, wfp_concat

    def make_motion_template(self, I_d_lst, c_d_eyes_lst, c_d_lip_lst, **kwargs):
        n_frames = I_d_lst.shape[0]
        template_dct = {
            'n_frames': n_frames,
            'output_fps': kwargs.get('output_fps', 25),
            'motion': [],
            'c_d_eyes_lst': [],
            'c_d_lip_lst': [],
        }

        for i in track(range(n_frames), description='Making motion templates...', total=n_frames):
            # collect s_d, R_d, Î´_d and t_d for inference
            I_d_i = I_d_lst[i]
            x_d_i_info = self.live_portrait_wrapper.get_kp_info(I_d_i)
            R_d_i = get_rotation_matrix(x_d_i_info['pitch'], x_d_i_info['yaw'], x_d_i_info['roll'])

            item_dct = {
                'scale': x_d_i_info['scale'].cpu().numpy().astype(np.float32),
                'R_d': R_d_i.cpu().numpy().astype(np.float32),
                'exp': x_d_i_info['exp'].cpu().numpy().astype(np.float32),
                't': x_d_i_info['t'].cpu().numpy().astype(np.float32),
            }

            template_dct['motion'].append(item_dct)

            c_d_eyes = c_d_eyes_lst[i].astype(np.float32)
            template_dct['c_d_eyes_lst'].append(c_d_eyes)

            c_d_lip = c_d_lip_lst[i].astype(np.float32)
            template_dct['c_d_lip_lst'].append(c_d_lip)

        return template_dct
